# üöÄ Guia: Iniciar Ollama e Testar Conex√£o

## üî¥ Problema Comum

```
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address
```

**Causa:** H√° outro processo Ollama j√° rodando na porta 11434.

---

## ‚úÖ Solu√ß√£o R√°pida

### Op√ß√£o 1: Usar o Script Autom√°tico (Recomendado)
```powershell
# Na pasta do projeto, execute:
cd C:\Users\tisuporte\Documents\jarvis_local

# Abra um terminal separado e execute:
.\start_ollama.bat
```

**O que faz:**
- ‚úÖ Para qualquer Ollama que esteja rodando
- ‚úÖ Aguarda a porta ficar dispon√≠vel  
- ‚úÖ Inicia um novo Ollama

### Op√ß√£o 2: Parar Manualmente (PowerShell - Admin)

```powershell
# Listar processos Ollama
Get-Process ollama -ErrorAction SilentlyContinue

# Parar todos os processos
Stop-Process -Name ollama -Force

# Aguardar
Start-Sleep -Seconds 3

# Iniciar Ollama
ollama serve
```

### Op√ß√£o 3: Parar Manualmente (CMD - Admin)

```cmd
taskkill /IM ollama.exe /F /T
timeout /t 3
ollama serve
```

---

## üß™ Testar Conex√£o

Ap√≥s iniciar Ollama, em outro terminal execute:

```powershell
cd C:\Users\tisuporte\Documents\jarvis_local\jarvis-core
python .\test_ollama.py
```

**Esperado:**
```
‚úÖ Ollama est√° rodando!
‚úÖ Modelos encontrados:
   üéØ qwen3:4b
‚úÖ Conex√£o OK!
‚úÖ TUDO OK! Ollama est√° pronto para usar
```

---

## üîÑ Workflow Completo

**Terminal 1 - Iniciar Ollama:**
```powershell
.\start_ollama.bat
```

**Terminal 2 - Testar (opcional):**
```powershell
cd jarvis-core
python .\test_ollama.py
```

**Terminal 3 - Executar JARVIS:**
```powershell
cd jarvis-core
python .\agent.py
```

---

## ‚ùå Troubleshooting

### Linux:
```bash
curl https://ollama.ai/install.sh | sh
```

## Passo 2: Baixar o Modelo Qwen3

```bash
ollama pull qwen3:4b
```

Isso pode levar alguns minutos dependendo da sua conex√£o.

## Passo 3: Verificar Instala√ß√£o

Execute o diagn√≥stico:

```bash
cd jarvis_local/jarvis-core
python diagnose_ollama.py
```

Voc√™ deve ver:
```
‚úÖ Ollama j√° est√° rodando!
‚úÖ qwen3:4b
‚úÖ Modelo 'qwen3:4b' funcionando!
‚úÖ TUDO PRONTO! Ollama est√° funcionando corretamente!
```

## Passo 4: Iniciar JARVIS

### Op√ß√£o A: Com Ollama em Background (Windows)

1. Duplo-clique em `start_ollama.bat`
   (Uma nova janela abrir√° com Ollama rodando)

2. Em outra janela PowerShell:
```bash
cd jarvis-core
python agent.py
```

### Op√ß√£o B: Com Ollama Manual (qualquer OS)

Terminal 1 (Ollama):
```bash
ollama serve
```

Terminal 2 (JARVIS):
```bash
cd jarvis-core
python agent.py'
```

## Troubleshooting

### Erro: "Modelo 'qwen3:4b' n√£o encontrado"
```bash
ollama pull qwen3:4b
```

### Erro: "N√£o foi poss√≠vel conectar ao Ollama"
Certifique-se que:
1. Ollama est√° rodando (`ollama serve`)
2. Porta 11434 est√° acess√≠vel
3. N√£o h√° firewall bloqueando

### Erro: "Read timed out"
- Seu modelo pode estar muito pesado
- Aumente o timeout em `config/settings.py`
- Ou use um modelo menor

### Fallback Autom√°tico

Se Ollama n√£o estiver dispon√≠vel, JARVIS usa:
- ‚úÖ Modo MOCK autom√°tico
- ‚úÖ Roteiro pr√©-definido para testes
- ‚úÖ Sem erros ou travamentos

## Performance

| Modelo | Tamanho | RAM M√≠nimo | Tempo Resposta |
|--------|---------|-----------|-----------------|
| qwen3:4b | 2.5 GB | 4 GB | 30-60s |
| neural-chat:7b | 4 GB | 8 GB | 60-120s |

## Alternativas

Se Ollama n√£o funcionar, use:

### OpenAI API
```python
OLLAMA_MODEL = "gpt-3.5-turbo"
USE_OPENAI = True
OPENAI_API_KEY = "sk-..."
```

### HuggingFace
```python
from transformers import pipeline
```

## Suporte

Para mais informa√ß√µes:
- Ollama: https://ollama.ai
- Qwen: https://github.com/QwenLM/Qwen
- JARVIS: Este projeto
