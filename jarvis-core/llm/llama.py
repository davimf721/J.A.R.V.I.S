import requests
import time
from config.settings import OLLAMA_URL, OLLAMA_MODEL
from memory.memory import recall_memory
from utils.helpers import load_profile


def check_ollama_available():
    """Verifica se Ollama est√° dispon√≠vel"""
    try:
        response = requests.get("http://localhost:11435/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False



def ask_llama(prompt: str) -> str:
    """
    Chama Ollama para gerar resposta.
    Sem fallback - lan√ßa erro se Ollama n√£o estiver dispon√≠vel.
    """
    # Carrega perfil do usu√°rio
    profile = load_profile()

    # Recupera mem√≥rias relevantes
    memories = recall_memory(prompt)
    memory_context = "\n".join(memories) if memories else "Nenhuma mem√≥ria relevante."

    # Prompt final enviado ao modelo
    full_prompt = f"""
PERFIL DO USU√ÅRIO:
{profile}

MEM√ìRIAS IMPORTANTES:
{memory_context}

INSTRU√á√ÉO:
{prompt}
"""

    # Verificar se Ollama est√° dispon√≠vel ANTES de tentar
    if not check_ollama_available():
        error_msg = """
‚ùå ERRO: Ollama n√£o est√° dispon√≠vel!

Para iniciar o Ollama, execute em outro terminal (PowerShell):
    $env:OLLAMA_HOST="127.0.0.1:11435" ; ollama serve

Ou execute o script de diagn√≥stico:
    python diagnose_ollama.py

Requisitos:
- Ollama deve estar rodando em http://localhost:11435
- Modelo '{OLLAMA_MODEL}' deve estar instalado
  (instale com: ollama pull {OLLAMA_MODEL})
""".format(OLLAMA_MODEL=OLLAMA_MODEL)
        raise RuntimeError(error_msg)

    try:
        print(f"\nü§ñ [LLM] Conectando a Ollama ({OLLAMA_MODEL})...")
        print(f"   ‚è≥ Aguardando resposta (pode levar alguns minutos)...")
        
        # Chamada ao Ollama com timeout maior (300 segundos = 5 minutos)
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": full_prompt,
                "stream": False
            },
            timeout=300
        )

        # Verificar status
        if response.status_code == 404:
            error_msg = f"""
‚ùå ERRO: Modelo '{OLLAMA_MODEL}' n√£o encontrado em Ollama!

Instale o modelo com:
    ollama pull {OLLAMA_MODEL}

Modelos dispon√≠veis:
    ollama list
"""
            raise RuntimeError(error_msg)
        
        if response.status_code != 200:
            error_msg = f"""
‚ùå ERRO: Ollama retornou status {response.status_code}

Resposta: {response.text}
"""
            raise RuntimeError(error_msg)

        data = response.json()

        # Valida√ß√£o defensiva
        if "response" not in data:
            error_msg = f"""
‚ùå ERRO: Resposta inv√°lida de Ollama

Dados recebidos: {data}
"""
            raise RuntimeError(error_msg)

        print(f"‚úÖ Resposta recebida de Ollama!\n")
        return data["response"].strip()

    except requests.exceptions.Timeout:
        error_msg = f"""
‚ùå ERRO: Timeout ao conectar com Ollama (300s excedido)

Poss√≠veis causas:
- Ollama est√° processando o modelo (processo muito lento)
- Sua m√°quina n√£o tem recursos suficientes para rodar '{OLLAMA_MODEL}'
- H√° problema com a rede ou Ollama travou

Recomenda√ß√µes:
1. Verifique a janela do Ollama - h√° alguma mensagem de erro?
2. Tente aumentar o timeout ou usar um modelo menor
3. Reinicie Ollama e tente novamente

Para reiniciar Ollama:
    Ctrl+C (na janela do Ollama)
    $env:OLLAMA_HOST="127.0.0.1:11435" ; ollama serve
"""
        raise RuntimeError(error_msg)
    
    except requests.exceptions.ConnectionError:
        error_msg = f"""
‚ùå ERRO: N√£o foi poss√≠vel conectar ao Ollama em {OLLAMA_URL}

Para iniciar Ollama:
    $env:OLLAMA_HOST="127.0.0.1:11435" ; ollama serve

Confirme que Ollama est√° rodando:
    ollama list
"""
        raise RuntimeError(error_msg)
    
    except Exception as e:
        error_msg = f"""
‚ùå ERRO inesperado ao chamar Ollama

Tipo: {type(e).__name__}
Mensagem: {str(e)}

Verifique se Ollama est√° rodando corretamente:
    ollama serve
"""
        raise RuntimeError(error_msg) from e
